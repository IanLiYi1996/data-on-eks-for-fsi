# This RayCluster configuration deploys a distributed training environment for Llama2
# using AWS Neuron SDK and RayTrain on Amazon EKS.

# ----------------------------------------------------------------------
# NOTE: For detailed deployment instructions, refer to the DoEKS website (https://awslabs.github.io/data-on-eks/docs/category/training-on-eks).
# ----------------------------------------------------------------------

# ----------------------------------------------------------------------
# NOTE: We are using the default namespace for this deployment because the fsx-claim PVC is created under the default namespace by the Terraform blueprint.
# If you want to deploy the cluster in a dedicated namespace, ensure that the FSX for Lustre file system is also created in the same namespace since PVCs are namespace-bound.
# ----------------------------------------------------------------------

# Docs for Volcano with KubeRay: https://docs.ray.io/en/master/cluster/kubernetes/k8s-ecosystem/volcano.html
apiVersion: scheduling.volcano.sh/v1beta1
kind: Queue
metadata:
  name: llm-training-queue
  namespace: llm
spec:
  weight: 1
  capability:
    cpu: '1000'
    memory: 1500Gi

---
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: kuberay-trn1
  namespace: llm
  labels:
    ray.io/scheduler-name: volcano
    volcano.sh/queue-name: llm-training-queue
spec:
  rayVersion: 2.35.0
  headGroupSpec:
    # Head Node Configuration
    # This section defines the specification for the Ray head pod.
    # The head node manages the cluster and provides services like the dashboard and GCS.
    template:
      spec:
        containers:
          - name: ray-head
            image: public.ecr.aws/data-on-eks/ray-pytorch-training-neuronx:latest # Replace with your Docker image URL
            imagePullPolicy: Always # Pull the latest image each time
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh", "-c", "ray stop"]  # Graceful shutdown of Ray processes
            ports:
              - containerPort: 8265
                name: dashboard   # Expose Ray dashboard
              - containerPort: 6379
                name: redis       # Expose Redis port
              - containerPort: 10001
                name: object-manager # Expose object manager port
            resources:
              requests:
                cpu: 2
                memory: 10Gi
            volumeMounts:
              - name: persistent-storage # Mount shared filesystem (FSx for Lustre)
                mountPath: /shared
        # Node Selector for Karpenter
        # Karpenter will provision this head pod on a node with the specified labels.
        nodeSelector:
          NodeGroupType: x86-cpu-karpenter
          type: karpenter
        volumes:
          - name: persistent-storage
            persistentVolumeClaim:
              claimName: fsx-claim-ray   # Reference the PVC for shared storage
    rayStartParams:
      dashboard-host: 0.0.0.0    # Make dashboard accessible

  workerGroupSpecs:
    # Worker Node Configuration
    # This section defines the specification for the Ray worker pods.
    # Worker nodes execute tasks and participate in distributed training.
    - groupName: workergroup
      replicas: 1  # Number of worker replicas
      minReplicas: 1 # Minimum number of worker replicas
      maxReplicas: 2 # Maximum number of worker replicas (no scaling in this case)
      rayStartParams: {}
      template:
        spec:
          containers:
            - name: ray-worker
              image: public.ecr.aws/data-on-eks/ray-pytorch-training-neuronx:latest # Replace with your Docker image URL
              imagePullPolicy: Always # Pull the latest image each time
              lifecycle:
                preStop:
                  exec:
                    command: ["/bin/sh", "-c", "ray stop"]
              ports:
                - containerPort: 8265
                  name: dashboard
                - containerPort: 6379
                  name: redis
                - containerPort: 10001
                  name: object-manager
              resources:
                limits:
                  aws.amazon.com/neuron: '1'  # Request AWS Neuron cores
                  memory: 32Gi
                requests:
                  aws.amazon.com/neuron: '1'
                  cpu: '8'
                  memory: 32Gi
              volumeMounts:
                - name: persistent-storage
                  mountPath: /shared   # Mount shared filesystem (FSx for Lustre)
                - name: dshm
                  mountPath: /dev/shm   # Mount for shared memory
          # Node Selector for Managed Node Group (with Cluster Autoscaler)
          # These workers will run on Trn1 instances provisioned by the cluster autoscaler.
          # This is necessary as Karpenter doesn't currently support EFA (required for Neuron distributed training).
          nodeSelector:
            NodeGroupType: trn1-resources-karpenter
            type: karpenter
          tolerations:
            - key: aws.amazon.com/neuroncore
              value: "true"
              effect: "NoSchedule"
            - key: aws.amazon.com/neuron
              value: "true"
              effect: "NoSchedule"
          volumes:
            # Persistent Volume Claim (PVC) to access the FSx for Lustre filesystem
            - name: persistent-storage
              persistentVolumeClaim:
                claimName: fsx-claim-ray
            - name: dshm
              emptyDir:
                medium: Memory
