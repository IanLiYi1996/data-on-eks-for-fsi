hub:
  command: [ "sh", "-c", "pip install boto3 kubernetes && pip install jupyterhub==4.1.0 && jupyterhub --config /usr/local/etc/jupyterhub/jupyterhub_config.py" ]
  config:
    Authenticator:
      admin_users:
        - admin1
      allowed_users:
        - user1
        - pinyusu
        - liyi
        - ianleely
    # testing only - do not do this for production
    DummyAuthenticator:
      password: never-do-this
    JupyterHub:
      authenticator_class: dummy
      http_timeout: 1200  # 与 singleuser.startTimeout 一致
    KubeSpawner:
      k8s_api_request_timeout: 240
  extraConfig:
    jupyterhub_config.py: |-
      from kubernetes import client, config
      def create_service_account(username):
          config.load_incluster_config()
          v1 = client.CoreV1Api()
          metadata = client.V1ObjectMeta(name=f"jupyter-{username}")
          service_account = client.V1ServiceAccount(metadata=metadata)
          try:
              v1.create_namespaced_service_account("jupyterhub", service_account)
          except client.exceptions.ApiException as e:
              if e.status == 409:  # Already exists
                  pass
              else:
                  raise

      def modify_pod_hook(spawner, pod):
          username = spawner.user.name
          create_service_account(username)
          pod.spec.service_account_name = f"jupyter-{username}"
          return pod

      c.KubeSpawner.modify_pod_hook = modify_pod_hook
      c.Spawner.start_timeout = 1200
      c.KubeSpawner.start_timeout = 1200
      c.Spawner.http_timeout = 1200  
      c.Spawner.args = ['--allow-root']
      c.KubeSpawner.notebook_dir = '/home'
#      c.KubeSpawner.enable_user_namespaces = true
#      c.KubeSpawner.user_namespace_template = 'jupyter-{username}'

proxy:
  service:
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
      service.beta.kubernetes.io/aws-load-balancer-scheme: internal # Private Load Balancer can only be accessed within the VPC
      service.beta.kubernetes.io/aws-load-balancer-type: external
      service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: 'true'
      service.beta.kubernetes.io/aws-load-balancer-ip-address-type: ipv4
singleuser:
  startTimeout: 1200
  profileList:
    - display_name: Data Engineering (CPU)
      description: "PySpark Notebooks | Karpenter AutoScaling"
      profile_options:
        image:
          display_name: "Image"
          choices:
            pyspark350:
              display_name: "PySpark 3.5.0 + Python 3.11"
              default: true
              kubespawner_override:
                image: jupyter/pyspark-notebook:spark-3.5.0
            pyspark341:
              display_name: "PySpark 3.4.1 + Python 3.11"
              kubespawner_override:
                image: jupyter/pyspark-notebook:spark-3.4.1
      kubespawner_override:
        node_selector:
          NodePool: x86-cpu-karpenter
        cpu_guarantee: 2
        mem_guarantee: 10G
        cpu_limit: 3.5
        mem_limit: 14G
      cmd: "start-singleuser.sh"
    - display_name: Trainium (trn1)
      description: "Trainium | Karpenter AutoScaling"
      profile_options:
        image:
          display_name: "Image"
          choices:
            pytorch1131:
              display_name: "PyTorch + torch-neuronx"
              default: true
              kubespawner_override:
                image: public.ecr.aws/data-on-eks/pytorch-neuronx:latest
      kubespawner_override:
        node_selector:
          NodePool: trainium
        tolerations:
          - key: aws.amazon.com/neuroncore
            operator: Exists
            effect: NoSchedule
          - key: aws.amazon.com/neuron
            operator: Exists
            effect: NoSchedule
        cpu_guarantee: 2
        mem_guarantee: 10G
        cpu_limit: 2
        mem_limit: 10G
        extra_resource_limits:
          aws.amazon.com/neuron: "1"
        cmd: "start-singleuser.sh"
    - display_name: Inferentia (inf2)
      description: "Inferentia | Karpenter AutoScaling"
      profile_options:
        image:
          display_name: "Image"
          choices:
            pytorch1131:
              display_name: "PyTorch + torch-neuronx"
              default: true
              kubespawner_override:
                image: public.ecr.aws/data-on-eks/pytorch-neuronx:latest
      kubespawner_override:
        node_selector:
          NodePool: inferentia
        tolerations:
          - key: aws.amazon.com/neuroncore
            operator: Exists
            effect: NoSchedule
          - key: aws.amazon.com/neuron
            operator: Exists
            effect: NoSchedule
        cpu_guarantee: 20
        mem_guarantee: 100G
        cpu_limit: 20
        mem_limit: 100G
        extra_resource_limits:
          aws.amazon.com/neuron: "1"
        cmd: null
    - display_name: Data Science (GPU - G4dn.2xlarge)
      description: "GPU with G4dn instances | Karpenter Autoscaler"
      kubespawner_override:
        image: cschranz/gpu-jupyter:v1.6_cuda-11.8_ubuntu-22.04_python-only
        node_selector:
          NodePool: g4dn-gpu-karpenter
          hub.jupyter.org/node-purpose: user
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
          - key: "hub.jupyter.org/dedicated" # According to optimization docs https://z2jh.jupyter.org/en/latest/administrator/optimization.html
            operator: "Equal"
            value: "user"
            effect: "NoSchedule"
        extra_resource_limits:
          nvidia.com/gpu: "1"
        cpu_guarantee: 4
        mem_guarantee: 16G
        cpu_limit: 7
        mem_limit: 30G
        cmd: null
    - display_name: Data Science (GPU - G5.2xlarge)
      description: "GPU with G5 instances | Karpenter Autoscaler"
      kubespawner_override:
        image: cschranz/gpu-jupyter:v1.6_cuda-11.8_ubuntu-22.04_python-only
        node_selector:
          NodePool: g5-gpu-karpenter
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
        extra_resource_limits:
          nvidia.com/gpu: "1"
        cpu_guarantee: 4
        mem_guarantee: 16G
        cpu_limit: 7
        mem_limit: 30G
        cmd: "start-singleuser.sh"
    - display_name: Data Science (GPU - G6.2xlarge)
      description: "GPU with G6 instances | Karpenter Autoscaler"
      kubespawner_override:
        image: cschranz/gpu-jupyter:v1.6_cuda-11.8_ubuntu-22.04_python-only
        node_selector:
          NodePool: g6-gpu-karpenter
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
        extra_resource_limits:
          nvidia.com/gpu: "1"
        cpu_guarantee: 4
        mem_guarantee: 16G
        cpu_limit: 7
        mem_limit: 30G
        cmd: "start-singleuser.sh"
    - display_name: Data Science (GPU - G6e Time-Slicing)
      description: "GPU with G6e instances Time-Slicing | Karpenter Autoscaler"
      kubespawner_override:
        image: cschranz/gpu-jupyter:v1.6_cuda-11.8_ubuntu-22.04_python-only
        node_selector:
          NodePool: g6e-gpu-karpenter-ts
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
        extra_resource_limits:
          nvidia.com/gpu: "1"
        cpu_guarantee: 4
        mem_guarantee: 16G
        cpu_limit: 7
        mem_limit: 30G
        cmd: "start-singleuser.sh"
    - display_name: Data Science (GPU - G6e.12xlage)
      description: "GPU with G6e instances | Karpenter Autoscaler"
      kubespawner_override:
        image: cschranz/gpu-jupyter:v1.6_cuda-11.8_ubuntu-22.04_python-only
        node_selector:
          NodePool: g6e-gpu-karpenter
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
        extra_resource_limits:
          nvidia.com/gpu: "4"
        cpu_guarantee: 32
        mem_guarantee: 300G
        cpu_limit: 40
        mem_limit: 320G
        cmd: "start-singleuser.sh"
#  extraEnv:
#    HUGGING_FACE_HUB_TOKEN:
#      valueFrom:
#        secretKeyRef:
#          name: hf-token
#          key: token
  storage:
    type: "static"
    static:
      pvcName: "efs-persist"
      subPath: "home/{username}"
    extraVolumes:
      - name: shm-volume
        emptyDir:
          medium: Memory
      - name: notebook
        configMap:
          name: notebook
      - name: jupyterhub-shared
        persistentVolumeClaim:
          claimName: efs-persist-shared
    extraVolumeMounts:
      - name: shm-volume
        mountPath: /dev/shm
      - name: notebook
        mountPath: /home/notebooks
      - name: jupyterhub-shared
        mountPath: /home/shared
        subPath: shared
        readOnly: false
#  serviceAccountName: ${jupyter_single_user_sa_name}
  allowPrivilegeEscalation: true
  extraPodConfig:
    securityContext:
      fsGroup: 100
  extraEnv: # Sudo needed to configure the proper permissions to start the notebook instance
    GRANT_SUDO: "yes"
    NOTEBOOK_ARGS: "--allow-root"
    CHOWN_HOME: "yes"
    CHOWN_HOME_OPTS: "-R"
#    CHOWN_EXTRA: "/home/"
  uid: 0
  fsGid: 0


# Optimizations configured according to this doc https://z2jh.jupyter.org/en/latest/administrator/optimization.html
scheduling:
  userScheduler:
    enabled: true
  podPriority:
    enabled: true
  userPlaceholder:
    enabled: false
    replicas: 1
  userPods:
    nodeAffinity:
      matchNodePurpose: require # This will force single-user pods to use an specific karpenter provisioner

prePuller:
  hook:
    enabled: false
  continuous:
    # NOTE: if used with Karpenter, also add user-placeholders
    enabled: false

global:
  safeToShowValues: false